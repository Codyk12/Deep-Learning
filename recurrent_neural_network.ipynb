{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Lab6.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "metadata": {
    "id": "9WnsXf8jhkeN",
    "colab_type": "code",
    "outputId": "403aa4c9-9aab-4634-ae5c-586cd89a6261",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    }
   },
   "cell_type": "code",
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch\n",
    "!pip install unidecode\n",
    "\n",
    "!pip install torchtext==0.2.3\n",
    "\n",
    "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
    "! tar -xzf text_files.tar.gz"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
      "\r\u001b[K    4% |█▍                              | 10kB 16.9MB/s eta 0:00:01\r\u001b[K    8% |██▉                             | 20kB 2.8MB/s eta 0:00:01\r\u001b[K    13% |████▏                           | 30kB 3.2MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 40kB 3.0MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 51kB 3.3MB/s eta 0:00:01\r\u001b[K    26% |████████▍                       | 61kB 3.9MB/s eta 0:00:01\r\u001b[K    30% |█████████▊                      | 71kB 3.6MB/s eta 0:00:01\r\u001b[K    34% |███████████▏                    | 81kB 3.6MB/s eta 0:00:01\r\u001b[K    39% |████████████▌                   | 92kB 4.1MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 102kB 4.2MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 112kB 4.3MB/s eta 0:00:01\r\u001b[K    52% |████████████████▊               | 122kB 5.0MB/s eta 0:00:01\r\u001b[K    56% |██████████████████              | 133kB 5.0MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▌            | 143kB 6.2MB/s eta 0:00:01\r\u001b[K    65% |████████████████████▉           | 153kB 6.3MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 163kB 5.2MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 174kB 6.4MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████       | 184kB 7.0MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 194kB 7.0MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▉    | 204kB 7.0MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▎  | 215kB 6.2MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▋ | 225kB 7.6MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 235kB 7.2MB/s \n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.0.22\n",
      "Collecting torchtext==0.2.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/90/474d5944d43001a6e72b9aaed5c3e4f77516fbef2317002da2096fd8b5ea/torchtext-0.2.3.tar.gz (42kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 3.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.2.3) (4.26.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.2.3) (2.18.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (2018.8.24)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (1.22)\n",
      "Building wheels for collected packages: torchtext\n",
      "  Running setup.py bdist_wheel for torchtext ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/42/a6/f4/b267328bde6bb680094a0c173e8e5627ccc99543abded97204\n",
      "Successfully built torchtext\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.2.3\n",
      "--2018-10-16 04:10:27--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
      "Resolving piazza.com (piazza.com)... 52.20.136.189, 52.55.184.84, 34.200.202.18, ...\n",
      "Connecting to piazza.com (piazza.com)|52.20.136.189|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
      "--2018-10-16 04:10:27--  https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
      "Resolving d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)... 13.33.227.29, 13.33.227.180, 13.33.227.25, ...\n",
      "Connecting to d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)|13.33.227.29|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1533290 (1.5M) [application/x-gzip]\n",
      "Saving to: ‘./text_files.tar.gz’\n",
      "\n",
      "./text_files.tar.gz 100%[===================>]   1.46M  9.71MB/s    in 0.2s    \n",
      "\n",
      "2018-10-16 04:10:27 (9.71 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {
    "id": "QbYzJheYhg2n",
    "colab_type": "code",
    "outputId": "c7933bd7-5a49-4068-f89f-c7eecd55c1b0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    " \n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " \n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    " \n",
    "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "file_len = 2579888\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {
    "id": "sD6cwQJ_kj0E",
    "colab_type": "code",
    "outputId": "8dccf1de-179d-4128-cd24-64e51d35bed7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    }
   },
   "cell_type": "code",
   "source": [
    "chunk_len = 200\n",
    " \n",
    "def random_chunk():\n",
    "  start_index = random.randint(0, file_len - chunk_len)\n",
    "  end_index = start_index + chunk_len + 1\n",
    "  return file[start_index:end_index]\n",
    " \n",
    "print(random_chunk())\n",
    "\n",
    "\n",
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "  tensor = torch.zeros(len(string)).long()\n",
    "  for c in range(len(string)):\n",
    "      tensor[c] = all_characters.index(string[c])\n",
    "  return Variable(tensor)\n",
    " \n",
    "print(char_tensor('abcDEF'))\n",
    " \n",
    "def random_training_set():    \n",
    "  chunk = random_chunk()\n",
    "  inp = char_tensor(chunk[:-1])\n",
    "  target = char_tensor(chunk[1:])\n",
    "  return inp, target"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "ards. It ran cunningly, taking a line that seemed chosen so as to keep \n",
      "as much hidden as possible from the view, both of the hill-tops above and of \n",
      "the flats to the west. It dived into dells, and hug\n",
      "tensor([10, 11, 12, 39, 40, 41])\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {
    "id": "_ZEufTDM00qR",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "#PART 3\n",
    "class GRU(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "    super(GRU, self).__init__()\n",
    "    \n",
    "    self.z_sig = nn.Sigmoid()\n",
    "    self.r_sig = nn.Sigmoid()\n",
    "    self.tanh = nn.Tanh()\n",
    "    \n",
    "    self.lin1 = nn.Linear(hidden_size, 100)\n",
    "    self.lin2 = nn.Linear(hidden_size, 100)\n",
    "   \n",
    " \n",
    "  def forward(self, input, hidden):\n",
    "    z = self.z_sig(torch.cat((hidden, input), 2))\n",
    "    print(z.size())\n",
    "    r = self.r_sig(torch.cat((hidden, input), 2))\n",
    "    print(r.size())\n",
    "    h_r = self.lin1(hidden * r)\n",
    "    h_hat = self.tanh(torch.cat((h_r, self.lin2(input)), 1))\n",
    "    print(h_hat.size())\n",
    "    return (1 - z) * hidden + z * h_hat\n",
    "    \n",
    "    "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "jdFznG8cDSw1",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "    super(RNN, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    \n",
    "    # encode using embedding layer\n",
    "    # set up GRU passing in number of layers parameter (nn.GRU)\n",
    "    self.GRU = GRU(input_size=input_size, hidden_size=hidden_size, num_layers=n_layers)\n",
    "    self.out = nn.Linear(hidden_size, output_size)\n",
    "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "    \n",
    "    # decode output\n",
    "\n",
    "  def forward(self, input, hidden):\n",
    "    # by reviewing the documentation, construct a forward function that properly uses the output\n",
    "    # of the GRU\n",
    "    # return output and hidden.\n",
    "    input = self.embedding(input).view(1, 1, -1) \n",
    "    output, hidden = self.GRU(input, hidden)\n",
    "    return  self.out(output[0]), hidden\n",
    "\n",
    "  def init_hidden(self):\n",
    "    return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "    "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "piMbszfYsUV0",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "def train(inp, target):\n",
    "  \n",
    "  hidden = decoder.init_hidden()\n",
    "  loss = 0\n",
    "  decoder_optimizer.zero_grad()\n",
    "  \n",
    "  \n",
    "  for c in range(chunk_len):\n",
    "    output, hidden = decoder(inp[c], hidden)# run the forward pass of your rnn with proper input\n",
    "    loss += criterion(output, target[c].view(1))\n",
    "\n",
    "  loss.backward()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item() / chunk_len"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "TQ_8j22rsXrZ",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "\n",
    "  prime_input = char_tensor(prime_str)\n",
    "  \n",
    "  hidden = decoder.init_hidden()\n",
    "  predicted = \"\"\n",
    "\n",
    "# Use priming string to \"build up\" hidden state\n",
    "  for p in range(len(prime_str) - 1):\n",
    "    _, hidden = decoder(prime_input[p], hidden)\n",
    "  inp = prime_input[-1]\n",
    "\n",
    "  for p in range(predict_len):\n",
    "    output, hidden = decoder(inp, hidden) #run your RNN/decoder forward on the input\n",
    "\n",
    "    # Sample from the network as a multinomial distribution\n",
    "    output_dist = output.data.view(-1).div(temperature).exp()\n",
    "    top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "    #top_i convert to char append\n",
    "\n",
    "    ## get character from your list of all characters, add it to your output str sequence, set input\n",
    "    predicted += all_characters[top_i]\n",
    "    inp = char_tensor(all_characters[top_i])\n",
    "\n",
    "  return predicted"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "print_every = 40\n",
    "plot_every = 15\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "lr = 0.005\n",
    " \n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  loss_ = train(*random_training_set())       \n",
    "  loss_avg += loss_\n",
    "\n",
    "  if epoch % print_every == 0:\n",
    "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
    "      print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "  if epoch % plot_every == 0:\n",
    "      all_losses.append((epoch, loss_avg / plot_every))\n",
    "      loss_avg = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "id": "Ym0xZ9x-0yaA",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "loss_x, loss_y = zip(*all_losses)\n",
    "plt.plot(loss_x, loss_y, label='Losses')\n",
    "plt.legend()\n",
    "plt.title(\"LOSS\")\n",
    "plt.show()\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "2Xz1HdoEMOwe",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    ""
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}