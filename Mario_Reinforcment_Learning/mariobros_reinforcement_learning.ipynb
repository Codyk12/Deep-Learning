{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "601_Project2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjG1LZuUfwi1",
        "colab_type": "code",
        "outputId": "8c524ab5-8a67-40bc-c862-58fd2cd3295e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt update\n",
        "!pip install gym-super-mario-bros\n",
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt-get install xvfb --fix-missing\n",
        "!pip install xvfbwrapper\n",
        "!apt install xvfb -y\n",
        "!pip install gym[atari]\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "!apt-get install x11-utils\n",
        "!apt install ffmpeg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:12 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [857 kB]\n",
            "Get:14 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,784 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,151 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [832 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,361 kB]\n",
            "Get:19 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [861 kB]\n",
            "Fetched 7,117 kB in 6s (1,215 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "25 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Collecting gym-super-mario-bros\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/b8/07460212c2568f78b02995834e7bdc25349e586473919e2983e01b984abf/gym_super_mario_bros-7.3.0-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 2.5MB/s \n",
            "\u001b[?25hCollecting nes-py>=8.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/98/f87eacc9ff3ddfe97ecc889165119317cd4782f5839c24b39f88a1a7e7d7/nes_py-8.1.1.tar.gz (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.10.9 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (1.18.2)\n",
            "Requirement already satisfied: pyglet>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.19.5 in /usr/local/lib/python3.6/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros) (4.38.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=8.0.0->gym-super-mario-bros) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.3.2->nes-py>=8.0.0->gym-super-mario-bros) (0.16.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.1.1-cp36-cp36m-linux_x86_64.whl size=447693 sha256=f4fb9337d086c8775130f871a96440306aaf1c2c0d7c259a5f31835e425f5336\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/d7/e4/0949e4c8947993c5555730a3b15f3cdc5a86507b95388dd608\n",
            "Successfully built nes-py\n",
            "Installing collected packages: nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.3.0 nes-py-8.1.1\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 2s (243 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 144542 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,266 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.4 [784 kB]\n",
            "Fetched 784 kB in 2s (388 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 146897 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting xvfbwrapper\n",
            "  Downloading https://files.pythonhosted.org/packages/57/b6/4920eabda9b49630dea58745e79f9919aba6408d460afe758bf6e9b21a04/xvfbwrapper-0.2.9.tar.gz\n",
            "Building wheels for collected packages: xvfbwrapper\n",
            "  Building wheel for xvfbwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xvfbwrapper: filename=xvfbwrapper-0.2.9-cp36-none-any.whl size=5010 sha256=e3d7266e3794185c7d175c7f9f201b583abdf5f26be9f99776b54600504269f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/f2/61/cacfaf84b352c223761ea8d19616e3b5ac5c27364da72863f0\n",
            "Successfully built xvfbwrapper\n",
            "Installing collected packages: xvfbwrapper\n",
            "Successfully installed xvfbwrapper-0.2.9\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/69/ec/8221a07850d69fa3c57c02e526edd23d18c7c05d58ed103e3b19172757c1/PyVirtualDisplay-0.2.5-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/32/8f/88d636f1da22a3c573259e44cfefb46a117d3f9432e2c98b1ab4a21372ad/EasyProcess-0.2.10-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.2.10 pyvirtualdisplay-0.2.5\n",
            "Collecting piglet\n",
            "  Downloading https://files.pythonhosted.org/packages/11/56/6840e5f45626dc7eb7cd5dff57d11880b3113723b3b7b1fb1fa537855b75/piglet-1.0.0-py2.py3-none-any.whl\n",
            "Collecting piglet-templates\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/02/8ff42edafaa1bdff3563f2bd5acb0f1cdc400ab0743f8cc71c463ea9bf9a/piglet_templates-0.5.1-py2.py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.3MB/s \n",
            "\u001b[?25hCollecting Parsley\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d6/4fed8d65e28a970e1c5cb33ce9c7e22e3de745e1b2ae37af051ef16aea3b/Parsley-1.3-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (19.3.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Collecting astunparse\n",
            "  Downloading https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.34.2)\n",
            "Installing collected packages: Parsley, astunparse, piglet-templates, piglet\n",
            "Successfully installed Parsley-1.3 astunparse-1.6.3 piglet-1.0.0 piglet-templates-0.5.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxxf86dga1 x11-utils\n",
            "0 upgraded, 2 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 209 kB of archives.\n",
            "After this operation, 711 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Fetched 209 kB in 2s (107 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 146904 files and directories currently installed.)\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nfx_sweaWFUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xvfbwrapper\n",
        "from pyvirtualdisplay import Display\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from functools import reduce\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "folder = \"./drive/My Drive/601R/project_2/recordings/\"\n",
        "p_model_path = \"./drive/My Drive/601R/project_2/models/policy1.pt\"\n",
        "v_model_path = \"./drive/My Drive/601R/project_2/models/value1.pt\"\n",
        "step_graph = \"./drive/My Drive/601R/project_2/graphs/steps1.png\"\n",
        "v_graph = \"./drive/My Drive/601R/project_2/graphs/v_loss1.png\"\n",
        "p_graph = \"./drive/My Drive/601R/project_2/graphs/p_loss1.png\"\n",
        "img_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf13UhfCV_Nk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "# env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "# env = gym.wrappers.Monitor(env, folder,force=True)\n",
        "\n",
        "# state = env.reset()\n",
        "# env.render()\n",
        "# img_size = 64\n",
        "\n",
        "# state = env.reset()\n",
        "# action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "# env.get_keys_to_action()\n",
        "# plt.imshow(env.render('rgb_array'))\n",
        "\n",
        "# x = resize(state).unsqueeze(0)\n",
        "# policy_net = Network(state_dim=img_size,action_dim=action_dim)\n",
        "# value_net = Network(state_dim=img_size)\n",
        "\n",
        "# y = value_net(x)\n",
        "# print(y)\n",
        "# print(env.action_space.sample())\n",
        "\n",
        "# done = True\n",
        "# for step in range(1):\n",
        "#     if done:\n",
        "#         state = env.reset()\n",
        "#     state, reward, done, info = env.step(env.action_space.sample())\n",
        "    # env.render('rgb_array')\n",
        "    # plt.imshow(env.render('rgb_array'))\n",
        "    # plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S8chpoY7eYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "    \n",
        "  def __init__(self, state_dim, action_dim=1):\n",
        "    \"\"\"\n",
        "    Takes in a state and decides which action to take based on the given state\n",
        "    \"\"\"\n",
        "    super(Network, self).__init__()\n",
        "    self.action_dim = action_dim\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "    self.bn2 = nn.BatchNorm2d(32)\n",
        "    self.conv3 = nn.Conv2d(32, 16, kernel_size=3, stride=1)\n",
        "    self.bn3 = nn.BatchNorm2d(16)\n",
        "\n",
        "    self.linear = nn.Linear(256, action_dim)\n",
        "    self.soft_max =nn.Softmax(dim=1)\n",
        "\n",
        "      \n",
        "  def forward(self, x):\n",
        "      x = F.relu(self.bn1( self.conv1(x)))\n",
        "      x = F.relu(self.bn2(self.conv2(x)))\n",
        "      x = F.relu(self.bn3(self.conv3(x)))\n",
        "      if self.action_dim == 1:\n",
        "          return self.linear(x.view(x.size(0), -1))\n",
        "      else:\n",
        "          return self.soft_max(self.linear(x.view(x.size(0), -1)))\n",
        "\n",
        "\n",
        "class AdvantageDataset(Dataset):   \n",
        "  \"\"\"\n",
        "  Takes care of the Data set for the advantages\n",
        "  \"\"\"\n",
        "  def __init__(self, experience):                                                                                                                 \n",
        "    super(AdvantageDataset, self).__init__()                                                                                                    \n",
        "    self._exp = experience                                                                                                                      \n",
        "    self._num_runs = len(experience)                                                                                                            \n",
        "    self._length = reduce(lambda acc, x: acc + len(x), experience, 0)                                                                           \n",
        "\n",
        "  def __getitem__(self, index):                                                                                                                   \n",
        "    idx = 0                                                                                                                                     \n",
        "    seen_data = 0                                                                                                                               \n",
        "    current_exp = self._exp[0]                                                                                                                  \n",
        "    while seen_data + len(current_exp) - 1 < index:                                                                                             \n",
        "        seen_data += len(current_exp)                                                                                                           \n",
        "        idx += 1                                                                                                                                \n",
        "        current_exp = self._exp[idx]                                                                                                            \n",
        "    chosen_exp = current_exp[index - seen_data]                                                                                                 \n",
        "    return chosen_exp[0], chosen_exp[4]                                                                                                         \n",
        "\n",
        "  def __len__(self):                                                                                                                              \n",
        "    return self._length                                                                                                                         \n",
        "  \n",
        "class PolicyDataset(Dataset):   \n",
        "  \"\"\"\n",
        "  Takes care of the policys \n",
        "  \"\"\"\n",
        "  def __init__(self, experience):                                                                                                                 \n",
        "    super(PolicyDataset, self).__init__()                                                                                                       \n",
        "    self._exp = experience                                                                                                                      \n",
        "    self._num_runs = len(experience)                                                                                                            \n",
        "    self._length = reduce(lambda acc, x: acc + len(x), experience, 0)                                                                           \n",
        "\n",
        "  def __getitem__(self, index):                                                                                                                   \n",
        "    idx = 0                                                                                                                                     \n",
        "    seen_data = 0                                                                                                                               \n",
        "    current_exp = self._exp[0]                                                                                                                  \n",
        "    while seen_data + len(current_exp) - 1 < index:                                                                                             \n",
        "        seen_data += len(current_exp)                                                                                                           \n",
        "        idx += 1                                                                                                                                \n",
        "        current_exp = self._exp[idx]                                                                                                            \n",
        "    chosen_exp = current_exp[index - seen_data]                                                                                                 \n",
        "    return chosen_exp                                                                                                                           \n",
        "\n",
        "  def __len__(self):                                                                                                                              \n",
        "    return self._length \n",
        "  \n",
        "\n",
        "class Calculate:\n",
        "  \"\"\"\n",
        "  Calculates the the returns and the advantages for each rollout\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def returns(self, rollouts, gamma):\n",
        "    for i, rollout in enumerate(rollouts):\n",
        "      projected_reward = 0\n",
        "      for j in range(len(rollout))[::-1]:\n",
        "        state, probabilities, action_id, reward = rollout[j]\n",
        "        projected_reward = reward + (gamma * projected_reward)\n",
        "        rollout[j] = [state, probabilities, action_id, reward, projected_reward]\n",
        "  \n",
        "  def advantages(self, rollouts, value):\n",
        "    for i, rollout in enumerate(rollouts):\n",
        "      for j, expererience in enumerate(rollout):\n",
        "        expected_reward = expererience[4]\n",
        "        state = expererience[0]\n",
        "        advantage = expected_reward - value(torch.from_numpy(np.ascontiguousarray(state)).float().unsqueeze(0).cuda()).cpu().squeeze(0).item()\n",
        "        rollout[j] = expererience + [advantage]\n",
        "\n",
        "resize = T.Compose([T.ToPILImage(),\n",
        "                        T.Resize((img_size,img_size), interpolation=Image.CUBIC),\n",
        "                        T.ToTensor()])\n",
        "\n",
        "def format_state(img):\n",
        "    return resize(img).unsqueeze(0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etB53FmfWAfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(e, env, policy_net, policy_optim, policy_loss, value_net,value_optim, value_loss):\n",
        "    \n",
        "    calculate = Calculate()\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    # Hyperparameters\n",
        "    num_rollouts = 8\n",
        "    episode_length = 5000\n",
        "    gamma = 0.9\n",
        "    value_epochs = 4\n",
        "    policy_epochs = 10\n",
        "    batch_size = 32\n",
        "    policy_batch_size = 256\n",
        "    epsilon = 0.2\n",
        " \n",
        "    # generate rollouts\n",
        "    print(\"Epoch: \" + str(e+1))\n",
        "    rollouts = []\n",
        "    steps = 0\n",
        "    state = env.reset()\n",
        "    print(\"Rollouts\")\n",
        "    for r in range(num_rollouts):\n",
        "        print(\"Rollout:\",r)\n",
        "        cur_rollout = []\n",
        "        state = env.reset()\n",
        "\n",
        "        done = False\n",
        "        while(not done):\n",
        "            # rollout for a certain number of steps\n",
        "            action = policy_net(format_state(state).cuda()).cpu().detach().numpy()\n",
        "            action_id = np.argmax(np.random.multinomial(1, action.reshape((action_dim))))\n",
        "            new_state, reward, done, _ = env.step(action_id)\n",
        "\n",
        "            cur_rollout.append([resize(state), action.reshape(-1), action_id, reward])\n",
        "            steps += 1\n",
        "            state = new_state\n",
        "\n",
        "        rollouts.append(cur_rollout)\n",
        "\n",
        "    avg_steps = steps / num_rollouts\n",
        "    print(avg_steps)\n",
        "            \n",
        "    # calculate the return for the rollouts\n",
        "    calculate.returns(rollouts, gamma)\n",
        "\n",
        "    # Approximate the value function\n",
        "    value_dataset = AdvantageDataset(rollouts)\n",
        "    value_loader = DataLoader(value_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "    value_loader = value_loader\n",
        "    value_losses = []\n",
        "    print(\"Train Value\")\n",
        "    for _ in range(value_epochs):\n",
        "        # train value network\n",
        "        total_loss = 0\n",
        "        for state, returns in value_loader:\n",
        "            value_optim.zero_grad()\n",
        "            returns = returns.unsqueeze(1).float()\n",
        "            expected_returns = value_net(state.cuda())\n",
        "            loss = value_loss(expected_returns, returns.cuda())\n",
        "            loss.backward()\n",
        "            total_loss += loss.cpu().item()\n",
        "            value_optim.step()\n",
        "        value_losses.append(total_loss)\n",
        "        print(\"Value Loss\", total_loss/value_epochs)\n",
        "        \n",
        "    # calculate the advantages for the rollouts \n",
        "    calculate.advantages(rollouts, value_net)\n",
        "    \n",
        "    del value_dataset\n",
        "\n",
        "    # Learn a policy\n",
        "    policy_dataset = PolicyDataset(rollouts)\n",
        "    policy_loader = DataLoader(policy_dataset, batch_size=policy_batch_size, shuffle=True, pin_memory=True)\n",
        "    policy_losses = []\n",
        "\n",
        "    print(\"Train Policy\")\n",
        "    for _ in range(policy_epochs):\n",
        "        # train policy network\n",
        "        total_loss = 0\n",
        "        for state, old_policy_dist, action_id, reward, return_reward, advantage in policy_loader:\n",
        "            policy_optim.zero_grad()\n",
        "            cur_batch_size = reward.size()[0]\n",
        "            advantage = return_reward.float()\n",
        "\n",
        "            new_policy_dist = policy_net(state.cuda()).cpu()\n",
        "\n",
        "            ratio = new_policy_dist[range(cur_batch_size), action_id] / old_policy_dist[range(cur_batch_size), action_id] \n",
        "\n",
        "            left = ratio * advantage\n",
        "            right = torch.clamp(ratio*advantage, 1 - epsilon, 1 + epsilon ) * advantage\n",
        "            policy_loss = -torch.mean(torch.min(left, right))\n",
        "            policy_loss.backward()\n",
        "            total_loss += policy_loss.cpu().item()\n",
        "            policy_optim.step()\n",
        "            \n",
        "            policy_losses.append(total_loss)\n",
        "        print(\"Policy Loss\", total_loss/policy_epochs)\n",
        "\n",
        "    torch.save(policy_net.state_dict(), p_model_path)\n",
        "    torch.save(value_net.state_dict(), v_model_path)\n",
        "\n",
        "    return policy_net, policy_optim, policy_loss, value_net, value_optim, value_loss, avg_steps, np.mean(value_losses), np.mean(policy_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmtUK2BHMqX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(epochs = 50, test=False):\n",
        "\n",
        "    env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "    env = gym.wrappers.Monitor(env, folder,force=True)\n",
        "\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    if os.path.exists(p_model_path):\n",
        "        policy_net = Network(state_dim=img_size,action_dim=action_dim)\n",
        "        policy_net.load_state_dict(torch.load(p_model_path))\n",
        "\n",
        "        value_net = Network(state_dim=img_size)\n",
        "        value_net.load_state_dict(torch.load(v_model_path))\n",
        "\n",
        "        if test:\n",
        "            return policy_net, env\n",
        "\n",
        "    else:\n",
        "        policy_net = Network(state_dim=img_size,action_dim=action_dim)\n",
        "        value_net = Network(state_dim=img_size)\n",
        "\n",
        "    policy_net = policy_net.cuda()\n",
        "    value_net = value_net.cuda()\n",
        "    policy_optim = optim.Adam(policy_net.parameters(), lr=1e-2, weight_decay=0.01)\n",
        "    value_optim = optim.Adam(value_net.parameters(), lr=1e-3, weight_decay=1)\n",
        "    value_loss = nn.MSELoss()\n",
        "    policy_loss = 0\n",
        "\n",
        "    epoch_losses = np.zeros((epochs,3))\n",
        "    for e in range(epochs):\n",
        "        policy_net, policy_optim, policy_loss, value_net, value_optim, value_loss, avg_steps, v_losses, p_losses  = train(e, env, policy_net, policy_optim, policy_loss, value_net, value_optim, value_loss)\n",
        "        epoch_losses[e,:] = [avg_steps, v_losses, p_losses]\n",
        "\n",
        "    plt.plot(epoch_losses[:,0])\n",
        "    plt.title(\"Avg Steps\")\n",
        "    plt.savefig(step_graph)\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(epoch_losses[:,1])\n",
        "    plt.title(\"Value Loss\")\n",
        "    plt.savefig(v_graph)\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(epoch_losses[:,2])\n",
        "    plt.title(\"Policy Loss\")\n",
        "    plt.savefig(p_graph)\n",
        "    plt.show()\n",
        "\n",
        "    return policy_net, env\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQCEE_yCOB0N",
        "colab_type": "code",
        "outputId": "716b7b85-858c-4097-fc56-12052e25d79f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "policy, env = train_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Rollouts\n",
            "Rollout: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Rollout: 7\n",
            "16589.875\n",
            "Train Value\n",
            "Value Loss 3030.884357985109\n",
            "Value Loss 3005.964477499947\n",
            "Value Loss 3018.252912676893\n",
            "Value Loss 3005.504095816985\n",
            "Train Policy\n",
            "Policy Loss -27.6978567417711\n",
            "Policy Loss -27.71456451378763\n",
            "Policy Loss -27.717876727879048\n",
            "Policy Loss -27.76019252538681\n",
            "Policy Loss -27.74788079187274\n",
            "Policy Loss -27.79940391276032\n",
            "Policy Loss -27.752671939879654\n",
            "Policy Loss -27.805617905035614\n",
            "Policy Loss -27.756637449935077\n",
            "Policy Loss -27.76128536127508\n",
            "Epoch: 2\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "17830.375\n",
            "Train Value\n",
            "Value Loss 3026.913679065183\n",
            "Value Loss 3028.1813949700445\n",
            "Value Loss 2978.1921028494835\n",
            "Value Loss 2958.345505528152\n",
            "Train Policy\n",
            "Policy Loss -24.16423335764557\n",
            "Policy Loss -24.212078139185905\n",
            "Policy Loss -24.161339030228554\n",
            "Policy Loss -24.06666219374165\n",
            "Policy Loss -24.12501909174025\n",
            "Policy Loss -24.100251176580787\n",
            "Policy Loss -24.13695824556053\n",
            "Policy Loss -24.113887391611932\n",
            "Policy Loss -24.190644728392364\n",
            "Policy Loss -24.178181130066513\n",
            "Epoch: 3\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "18217.125\n",
            "Train Value\n",
            "Value Loss 2900.5571110527962\n",
            "Value Loss 2852.449217265472\n",
            "Value Loss 2824.7380643822253\n",
            "Value Loss 2841.5671308264136\n",
            "Train Policy\n",
            "Policy Loss -22.52033277284354\n",
            "Policy Loss -22.54813660569489\n",
            "Policy Loss -22.55581676964648\n",
            "Policy Loss -22.586310406401754\n",
            "Policy Loss -22.60237948000431\n",
            "Policy Loss -22.664696412719785\n",
            "Policy Loss -22.566816040780395\n",
            "Policy Loss -22.658725103363395\n",
            "Policy Loss -22.58529151622206\n",
            "Policy Loss -22.58885698989034\n",
            "Epoch: 4\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "19780.0\n",
            "Train Value\n",
            "Value Loss 3053.58941411227\n",
            "Value Loss 3027.1869280096143\n",
            "Value Loss 3010.524067124352\n",
            "Value Loss 2917.9898154158145\n",
            "Train Policy\n",
            "Policy Loss -19.42287497073412\n",
            "Policy Loss -19.51370411850512\n",
            "Policy Loss -19.469300531595945\n",
            "Policy Loss -19.5134471103549\n",
            "Policy Loss -19.67928682602942\n",
            "Policy Loss -19.49982970878482\n",
            "Policy Loss -19.695788002572954\n",
            "Policy Loss -19.519042919017373\n",
            "Policy Loss -19.58704707380384\n",
            "Policy Loss -19.49183332622051\n",
            "Epoch: 5\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "15797.75\n",
            "Train Value\n",
            "Value Loss 2883.396074878052\n",
            "Value Loss 2833.538789715618\n",
            "Value Loss 2833.9091892447323\n",
            "Value Loss 2800.9492576587945\n",
            "Train Policy\n",
            "Policy Loss -19.78146235514432\n",
            "Policy Loss -19.946022101119162\n",
            "Policy Loss -19.870746209844945\n",
            "Policy Loss -19.82385944072157\n",
            "Policy Loss -19.90507318265736\n",
            "Policy Loss -19.898152540344746\n",
            "Policy Loss -19.92566480077803\n",
            "Policy Loss -19.863809473626315\n",
            "Policy Loss -19.913404618948697\n",
            "Policy Loss -19.909176070988178\n",
            "Epoch: 6\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "18852.5\n",
            "Train Value\n",
            "Value Loss 2999.158710407093\n",
            "Value Loss 3045.2966119907796\n",
            "Value Loss 3038.429150959477\n",
            "Value Loss 3005.4502133969218\n",
            "Train Policy\n",
            "Policy Loss -19.903231384977698\n",
            "Policy Loss -20.10323540866375\n",
            "Policy Loss -20.06740924278274\n",
            "Policy Loss -20.054613701067865\n",
            "Policy Loss -19.914612287282942\n",
            "Policy Loss -20.0193056140095\n",
            "Policy Loss -20.180639160051943\n",
            "Policy Loss -19.94884563740343\n",
            "Policy Loss -20.036264109425247\n",
            "Policy Loss -20.001736666448416\n",
            "Epoch: 7\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "19243.25\n",
            "Train Value\n",
            "Value Loss 3317.087041310966\n",
            "Value Loss 3214.287822567858\n",
            "Value Loss 3246.352107421495\n",
            "Value Loss 3201.6035416144878\n",
            "Train Policy\n",
            "Policy Loss -28.443533423915504\n",
            "Policy Loss -28.36391752511263\n",
            "Policy Loss -28.394750987272708\n",
            "Policy Loss -28.393169414624573\n",
            "Policy Loss -28.443777791783212\n",
            "Policy Loss -28.408866282366215\n",
            "Policy Loss -28.355214432254435\n",
            "Policy Loss -28.424295128136873\n",
            "Policy Loss -28.43936336133629\n",
            "Policy Loss -28.4521222807467\n",
            "Epoch: 8\n",
            "Rollouts\n",
            "Rollout: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "19430.25\n",
            "Train Value\n",
            "Value Loss 3379.0826885662973\n",
            "Value Loss 3223.28149336949\n",
            "Value Loss 3242.3252900559455\n",
            "Value Loss 3259.255781468004\n",
            "Train Policy\n",
            "Policy Loss -23.0988364668563\n",
            "Policy Loss -23.25601793024689\n",
            "Policy Loss -23.102006452530624\n",
            "Policy Loss -23.295649197883904\n",
            "Policy Loss -23.182516769692302\n",
            "Policy Loss -23.270656463690102\n",
            "Policy Loss -23.095035139657558\n",
            "Policy Loss -23.26186973005533\n",
            "Policy Loss -23.201495306938888\n",
            "Policy Loss -23.242001285776496\n",
            "Epoch: 9\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "16073.375\n",
            "Train Value\n",
            "Value Loss 2814.4238365516067\n",
            "Value Loss 2697.74330647476\n",
            "Value Loss 2713.1218728534877\n",
            "Value Loss 2726.6585514247417\n",
            "Train Policy\n",
            "Policy Loss -22.784914322011172\n",
            "Policy Loss -22.941901949420572\n",
            "Policy Loss -22.870045178383588\n",
            "Policy Loss -22.86992472074926\n",
            "Policy Loss -22.84001420568675\n",
            "Policy Loss -22.91745574977249\n",
            "Policy Loss -22.957351102307438\n",
            "Policy Loss -22.958048840239645\n",
            "Policy Loss -22.887048308178784\n",
            "Policy Loss -22.931984059512615\n",
            "Epoch: 10\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "17661.125\n",
            "Train Value\n",
            "Value Loss 2885.0039910441265\n",
            "Value Loss 2867.301179853268\n",
            "Value Loss 2805.9313423167914\n",
            "Value Loss 2890.8157249204814\n",
            "Train Policy\n",
            "Policy Loss -24.28301545418799\n",
            "Policy Loss -24.307196495868265\n",
            "Policy Loss -24.32104757539928\n",
            "Policy Loss -24.377646808326244\n",
            "Policy Loss -24.267802282981574\n",
            "Policy Loss -24.32113297842443\n",
            "Policy Loss -24.382850869372486\n",
            "Policy Loss -24.296622623875738\n",
            "Policy Loss -24.38330120872706\n",
            "Policy Loss -24.289301894418895\n",
            "Epoch: 11\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "15860.375\n",
            "Train Value\n",
            "Value Loss 3029.2211856618524\n",
            "Value Loss 2954.9769617524\n",
            "Value Loss 2901.0158045478165\n",
            "Value Loss 2957.3763355463743\n",
            "Train Policy\n",
            "Policy Loss -25.348690745979546\n",
            "Policy Loss -25.401401352882385\n",
            "Policy Loss -25.47570713981986\n",
            "Policy Loss -25.494890257157387\n",
            "Policy Loss -25.381119175069035\n",
            "Policy Loss -25.453833403997123\n",
            "Policy Loss -25.43591205086559\n",
            "Policy Loss -25.459079001657663\n",
            "Policy Loss -25.418082108814268\n",
            "Policy Loss -25.476382738351823\n",
            "Epoch: 12\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "17199.75\n",
            "Train Value\n",
            "Value Loss 3195.942752717063\n",
            "Value Loss 3136.91647102125\n",
            "Value Loss 3138.267905762419\n",
            "Value Loss 3130.6104115303606\n",
            "Train Policy\n",
            "Policy Loss -26.57014592560008\n",
            "Policy Loss -26.768723806738855\n",
            "Policy Loss -26.698859153687955\n",
            "Policy Loss -26.722832320630552\n",
            "Policy Loss -26.71015577185899\n",
            "Policy Loss -26.64504728913307\n",
            "Policy Loss -26.68275930862874\n",
            "Policy Loss -26.758214979991315\n",
            "Policy Loss -26.74518298264593\n",
            "Policy Loss -26.725390788726507\n",
            "Epoch: 13\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "12632.125\n",
            "Train Value\n",
            "Value Loss 2616.3656241470017\n",
            "Value Loss 2563.257054711721\n",
            "Value Loss 2545.8510244945064\n",
            "Value Loss 2524.8668653499335\n",
            "Train Policy\n",
            "Policy Loss -28.790807082131504\n",
            "Policy Loss -28.802652346342803\n",
            "Policy Loss -28.82760983556509\n",
            "Policy Loss -28.870337728224694\n",
            "Policy Loss -28.816136705875397\n",
            "Policy Loss -28.82066203095019\n",
            "Policy Loss -28.86554667316377\n",
            "Policy Loss -28.825826190412045\n",
            "Policy Loss -28.87070171609521\n",
            "Policy Loss -28.864343269169332\n",
            "Epoch: 14\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Rollout: 7\n",
            "17052.625\n",
            "Train Value\n",
            "Value Loss 3041.5721716433764\n",
            "Value Loss 3024.775761567056\n",
            "Value Loss 3039.58279942628\n",
            "Value Loss 3012.1719038318843\n",
            "Train Policy\n",
            "Policy Loss -27.9773456659168\n",
            "Policy Loss -28.136329458095133\n",
            "Policy Loss -28.09306268133223\n",
            "Policy Loss -28.104107136279346\n",
            "Policy Loss -28.108485999330878\n",
            "Policy Loss -28.07700750492513\n",
            "Policy Loss -28.0726795155555\n",
            "Policy Loss -28.088499069213867\n",
            "Policy Loss -28.046789860352874\n",
            "Policy Loss -28.12728202007711\n",
            "Epoch: 15\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "12171.375\n",
            "Train Value\n",
            "Value Loss 2979.0395720228553\n",
            "Value Loss 2889.9591424055398\n",
            "Value Loss 2871.2472942788154\n",
            "Value Loss 2835.1297132540494\n",
            "Train Policy\n",
            "Policy Loss -34.349830695986746\n",
            "Policy Loss -34.408799819648266\n",
            "Policy Loss -34.392801500484346\n",
            "Policy Loss -34.42260563969612\n",
            "Policy Loss -34.43206027522683\n",
            "Policy Loss -34.44048668462783\n",
            "Policy Loss -34.38338605314493\n",
            "Policy Loss -34.469825581088664\n",
            "Policy Loss -34.35995383039117\n",
            "Policy Loss -34.42896570265293\n",
            "Epoch: 16\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "13205.125\n",
            "Train Value\n",
            "Value Loss 2948.437075117603\n",
            "Value Loss 2962.5629557352513\n",
            "Value Loss 2961.2437845338136\n",
            "Value Loss 2974.7949848575518\n",
            "Train Policy\n",
            "Policy Loss -30.267821387946604\n",
            "Policy Loss -30.351577010378243\n",
            "Policy Loss -30.287938076257706\n",
            "Policy Loss -30.35361026339233\n",
            "Policy Loss -30.264231130853297\n",
            "Policy Loss -30.309390173107385\n",
            "Policy Loss -30.252048893272878\n",
            "Policy Loss -30.30574324466288\n",
            "Policy Loss -30.363939472660423\n",
            "Policy Loss -30.312126801162957\n",
            "Epoch: 17\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "18718.75\n",
            "Train Value\n",
            "Value Loss 3520.829349000007\n",
            "Value Loss 3458.4530268553644\n",
            "Value Loss 3451.3337210249156\n",
            "Value Loss 3417.656419172883\n",
            "Train Policy\n",
            "Policy Loss -30.047816157713534\n",
            "Policy Loss -30.066459024697544\n",
            "Policy Loss -30.066283137723804\n",
            "Policy Loss -30.07375338971615\n",
            "Policy Loss -30.043894633837045\n",
            "Policy Loss -30.105558075755834\n",
            "Policy Loss -30.113043304532766\n",
            "Policy Loss -30.008599987998604\n",
            "Policy Loss -30.12955167600885\n",
            "Policy Loss -30.030595114640892\n",
            "Epoch: 18\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "16557.625\n",
            "Train Value\n",
            "Value Loss 3157.9964496791363\n",
            "Value Loss 3130.5874497350305\n",
            "Value Loss 3180.878965497017\n",
            "Value Loss 3133.4115611258894\n",
            "Train Policy\n",
            "Policy Loss -28.27946779243648\n",
            "Policy Loss -28.301827810332178\n",
            "Policy Loss -28.415447304863484\n",
            "Policy Loss -28.286911116167904\n",
            "Policy Loss -28.343022207356988\n",
            "Policy Loss -28.25089464262128\n",
            "Policy Loss -28.19102702066302\n",
            "Policy Loss -28.40952235199511\n",
            "Policy Loss -28.258236180990934\n",
            "Policy Loss -28.348067264072597\n",
            "Epoch: 19\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "12210.0\n",
            "Train Value\n",
            "Value Loss 3141.975882679224\n",
            "Value Loss 2983.1560850981623\n",
            "Value Loss 2987.8866749126464\n",
            "Value Loss 2967.747207103297\n",
            "Train Policy\n",
            "Policy Loss -35.069271652400495\n",
            "Policy Loss -35.214052964746955\n",
            "Policy Loss -35.17817912250757\n",
            "Policy Loss -35.194333103299144\n",
            "Policy Loss -35.246256521344186\n",
            "Policy Loss -35.159406455606224\n",
            "Policy Loss -35.226977141946556\n",
            "Policy Loss -35.18305319398642\n",
            "Policy Loss -35.2081998206675\n",
            "Policy Loss -35.170797844976185\n",
            "Epoch: 20\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n",
            "Rollout: 5\n",
            "Rollout: 6\n",
            "Rollout: 7\n",
            "19170.0\n",
            "Train Value\n",
            "Value Loss 3211.7593042701483\n",
            "Value Loss 3277.8684124313295\n",
            "Value Loss 3241.323912322521\n",
            "Value Loss 3299.9296416528523\n",
            "Train Policy\n",
            "Policy Loss -26.850846682675183\n",
            "Policy Loss -27.069208737183363\n",
            "Policy Loss -27.12377926260233\n",
            "Policy Loss -27.046603738330305\n",
            "Policy Loss -27.17080184388906\n",
            "Policy Loss -27.119137097336353\n",
            "Policy Loss -27.149498128145932\n",
            "Policy Loss -27.27086722124368\n",
            "Policy Loss -27.068874400481583\n",
            "Policy Loss -27.0071087339893\n",
            "Epoch: 21\n",
            "Rollouts\n",
            "Rollout: 0\n",
            "Rollout: 1\n",
            "Rollout: 2\n",
            "Rollout: 3\n",
            "Rollout: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj17Fd2aOEVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model():\n",
        "    policy_net, env = train_model(test=True)\n",
        "    policy_net.eval()\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    while(not done):\n",
        "        # rollout for a certain number of steps\n",
        "        action = policy_net(format_state(state)).detach().numpy()\n",
        "        action_id = np.argmax(np.random.multinomial(1, action.reshape((action_dim))))\n",
        "        state, reward, done, _ = env.step(action_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcPwhZgjdtUv",
        "colab_type": "code",
        "outputId": "0b03e361-72c1-47a9-f263-e51250aa3fa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "run_model()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjhA2T01W0tS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}